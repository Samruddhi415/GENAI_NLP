{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMDpn8pTuGCkUb68PwUkID3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"iudRtx1hmXOH"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import math\n","from torch.utils.data import DataLoader, TensorDataset"]},{"cell_type":"code","source":["def scaled_dot_product_attention(query, key, value, mask=None):\n","    dim_k = query.shape[-1]\n","    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(dim_k)\n","    if mask is not None:\n","        scores = scores.masked_fill(mask == 0, -1e9)\n","    attention = torch.nn.functional.softmax(scores, dim=-1)\n","    return torch.matmul(attention, value)\n"],"metadata":{"id":"ybbHDl4qmevM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, embed_size, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        self.num_heads = num_heads\n","        self.head_dim = embed_size // num_heads\n","        self.query = nn.Linear(embed_size, embed_size)\n","        self.key = nn.Linear(embed_size, embed_size)\n","        self.value = nn.Linear(embed_size, embed_size)\n","        self.fc_out = nn.Linear(embed_size, embed_size)\n","\n","    def forward(self, query, key, value, mask=None):\n","        batch_size = query.shape[0]\n","        Q = self.query(query).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n","        K = self.key(key).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n","        V = self.value(value).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n","        out = scaled_dot_product_attention(Q, K, V, mask)\n","        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.head_dim)\n","        return self.fc_out(out)"],"metadata":{"id":"ni6mt3q1mxnj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FeedForward(nn.Module):\n","    def __init__(self, embed_size, hidden_dim):\n","        super(FeedForward, self).__init__()\n","        self.fc1 = nn.Linear(embed_size, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, embed_size)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        return self.fc2(self.relu(self.fc1(x)))"],"metadata":{"id":"9wyzJvD9m0zc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, embed_size, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        pe = torch.zeros(max_len, embed_size)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-math.log(10000.0) / embed_size))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        self.pe = pe.unsqueeze(0)\n","\n","    def forward(self, x):\n","        return x + self.pe[:, :x.size(1)].to(x.device)"],"metadata":{"id":"Y4rpmT4jm4pw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TransformerEncoderLayer(nn.Module):\n","    def __init__(self, embed_size, num_heads, hidden_dim, dropout=0.1):\n","        super(TransformerEncoderLayer, self).__init__()\n","        self.attention = MultiHeadAttention(embed_size, num_heads)\n","        self.norm1 = nn.LayerNorm(embed_size)\n","        self.norm2 = nn.LayerNorm(embed_size)\n","        self.ffn = FeedForward(embed_size, hidden_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask=None):\n","        attn = self.attention(x, x, x, mask)\n","        x = self.norm1(x + self.dropout(attn))\n","        ffn_out = self.ffn(x)\n","        return self.norm2(x + self.dropout(ffn_out))"],"metadata":{"id":"3y9KPLhkm7Ng"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TransformerDecoderLayer(nn.Module):\n","    def __init__(self, embed_size, num_heads, hidden_dim, dropout=0.1):\n","        super(TransformerDecoderLayer, self).__init__()\n","        self.attention1 = MultiHeadAttention(embed_size, num_heads)\n","        self.attention2 = MultiHeadAttention(embed_size, num_heads)\n","        self.norm1 = nn.LayerNorm(embed_size)\n","        self.norm2 = nn.LayerNorm(embed_size)\n","        self.norm3 = nn.LayerNorm(embed_size)\n","        self.ffn = FeedForward(embed_size, hidden_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, enc_out, src_mask=None, tgt_mask=None):\n","        attn1 = self.attention1(x, x, x, tgt_mask)\n","        x = self.norm1(x + self.dropout(attn1))\n","        attn2 = self.attention2(x, enc_out, enc_out, src_mask)\n","        x = self.norm2(x + self.dropout(attn2))\n","        ffn_out = self.ffn(x)\n","        return self.norm3(x + self.dropout(ffn_out))"],"metadata":{"id":"Legear3km9_S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_transformer(model, dataset, epochs=10, batch_size=32, lr=0.001):\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    for epoch in range(epochs):\n","        for src, tgt in dataloader:\n","            optimizer.zero_grad()\n","            output = model(src, tgt)\n","            loss = criterion(output.view(-1, output.shape[-1]), tgt.view(-1))\n","            loss.backward()\n","            optimizer.step()\n","        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")"],"metadata":{"id":"6Ko6hB2_m_Ex"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_dim = 10000  # Vocabulary size\n","embed_size = 512\n","num_heads = 8\n","num_layers = 6\n","hidden_dim = 2048\n","output_dim = 10000"],"metadata":{"id":"wC0hRjVsnDSe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Transformer(nn.Module):\n","    def __init__(self, input_dim, embed_size, num_heads, num_layers, hidden_dim, output_dim):\n","        super(Transformer, self).__init__()\n","        self.encoder = nn.ModuleList([TransformerEncoderLayer(embed_size, num_heads, hidden_dim) for _ in range(num_layers)])\n","        self.decoder = nn.ModuleList([TransformerDecoderLayer(embed_size, num_heads, hidden_dim) for _ in range(num_layers)])\n","        self.embedding = nn.Embedding(input_dim, embed_size)\n","        self.pos_encoding = PositionalEncoding(embed_size)\n","        self.fc_out = nn.Linear(embed_size, output_dim)\n","\n","    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n","        src = self.embedding(src)\n","        src = self.pos_encoding(src)\n","        tgt = self.embedding(tgt)\n","        tgt = self.pos_encoding(tgt)\n","\n","        for layer in self.encoder:\n","            src = layer(src, src_mask)\n","\n","        for layer in self.decoder:\n","            tgt = layer(tgt, src, src_mask, tgt_mask)\n","\n","        return self.fc_out(tgt)"],"metadata":{"id":"Bx0Jy9MOnU_P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Transformer(input_dim, embed_size, num_heads, num_layers, hidden_dim, output_dim)\n","dataset = TensorDataset(torch.randint(0, input_dim, (1000, 20)), torch.randint(0, input_dim, (1000, 20)))\n","train_transformer(model, dataset)"],"metadata":{"id":"rgJcievEnHCi"},"execution_count":null,"outputs":[]}]}